{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "uof_page = requests.get(\"https://sbcountyda.org/categories/news-releases/use-of-force-reviews/page/1\").content\n",
    "soup = BeautifulSoup(uof_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_uof_catalog_pages():\n",
    "    \"\"\"\n",
    "    Helper method to get use of force catalog pages from San Bernardino site. Fetches base page, calculates number of pages based on results param located at top of page under \"<numeric> results\"\n",
    "    \"\"\"\n",
    "    # Get the number of pages from the \"post-results field on the \"Category: Use of Force Reviews\" page\n",
    "    try:\n",
    "        uof_page = requests.get(\n",
    "            \"https://sbcountyda.org/categories/news-releases/use-of-force-reviews/\"\n",
    "        ).content\n",
    "        soup = BeautifulSoup(uof_page)\n",
    "        # 10 links shown per page\n",
    "        number_of_pages = (\n",
    "            int(\n",
    "                re.search(\n",
    "                    \"\\d*\", soup.find_all(\"div\", {\"class\": \"post-results\"})[0].text\n",
    "                ).group()\n",
    "            )\n",
    "            % 10\n",
    "        )\n",
    "        # Generate a list of base pages that we'll need to call for pdf reports\n",
    "        base_pages = [\n",
    "            \"https://sbcountyda.org/categories/news-releases/use-of-force-reviews/page/{}/\".format(\n",
    "                i\n",
    "            )\n",
    "            for i in range(1, number_of_pages + 1)\n",
    "        ]\n",
    "        return base_pages\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise SystemExit(e)\n",
    "\n",
    "\n",
    "# TODO : Wrap the request in a try/catch\n",
    "def _get_uof_review_links():\n",
    "    \"\"\"\n",
    "    For each instance of UOF documented by the court, there is an individual page stored on the SB website. This method gives those links, which will then have a link to a pdf of the court doc and police report.\n",
    "    \"\"\"\n",
    "    # Get links to each individiual uof case filing page\n",
    "    catalog_pages = _get_uof_catalog_pages()\n",
    "    uof_links = []\n",
    "    # For each catalog page, we want to grab the set of individual links on each.\n",
    "    for cat in catalog_pages:\n",
    "        # We try to request the catalog page 3 times\n",
    "        tries = 3\n",
    "        for try_value in range(tries):\n",
    "            try:\n",
    "                ind_uof_page = requests.get(cat).content\n",
    "                soup = BeautifulSoup(ind_uof_page)\n",
    "                for i in soup.find_all(\n",
    "                    \"h2\", {\"class\": \"entry-title bolt-highlight-font\"}\n",
    "                ):\n",
    "                    regex_match = re.search(r\"\\<a href=.* rel\", str(i)).group()\n",
    "                    # Strip head and tail of match to get just the link. There's probably a better way to search this with bs4\n",
    "                    link_cleaned = (\n",
    "                        str(regex_match).replace('<a href=\"', \"\").replace('\" rel', \"\")\n",
    "                    )\n",
    "                    uof_links.append(link_cleaned)\n",
    "                # Courtesy sleep timer\n",
    "                time.sleep(1)\n",
    "            except KeyError as e:\n",
    "                time.sleep(1)\n",
    "                if try_value < tries - 1:  # i is zero indexed\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "            break\n",
    "    return uof_links\n",
    "\n",
    "\n",
    "# TODO : We need to write a function that can grab the .pdf's from each individual use of force case page. Not all pages will have the same format for .pdfs, so we need to search/grep them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat = _get_uof_review_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(rat[0]).content\n",
    "soup = BeautifulSoup(html)\n",
    "pdf_links = set()\n",
    "current_link = \"\"\n",
    "for i in set(soup.find_all(\"a\")):\n",
    "    temp_link = i.get(\"href\")\n",
    "    # For each distinct pdf, we want to add their link to the set of pdf's that we'll scrape\n",
    "    if temp_link.endswith(\"pdf\"):\n",
    "        pdf_links.add(temp_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://sbcountyda.org/wp-content/uploads/sites/42/2023/01/Public-Release-Memo-FINAL-Brandon-Rocky.pdf'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#primary\n"
     ]
    }
   ],
   "source": [
    "html = requests.get(rat[0]).content\n",
    "soup = BeautifulSoup(html)\n",
    "print(soup.find_all(\"a\")[0].get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data/\"):\n",
    "    os.mkdir(\"./data/\")\n",
    "\n",
    "# REPLACE rat WITH UOF link list\n",
    "for link in pdf_links:\n",
    "    # We try to request the uof document page 3 times\n",
    "    tries = 3\n",
    "    for try_value in range(tries):\n",
    "        try:\n",
    "            with open(\"./data/{}\".format(Path(link).name), \"wb\") as f:\n",
    "                response = requests.get(link)\n",
    "                f.write(response.content)\n",
    "            # Courtesy sleep timer\n",
    "            time.sleep(1)\n",
    "        except KeyError as e:\n",
    "            time.sleep(1)\n",
    "            if try_value < tries - 1:  # i is zero indexed\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdap-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
